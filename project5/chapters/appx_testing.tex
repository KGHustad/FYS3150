\section{Asserting the correctness of the program}
\label{appx:testing}

Testing a bilingual software project such as ours is slightly more demanding than testing software written in a single language. The errors may lay in code written in either of the languages or even in the interfacing layer. We therefore conduct a variety of tests to ensure that all of our code is correct.

From Python, we assert that our algorithms are correctly implemented with standard unit tests as described in \ref{appx:testing:unit_tests}. Additionally, we have doctests which have the strength of being easy to read and also make it easy to see where the test failed and what the expected and computed values were.

In C, we check that there are no memory related errors by writing simple test programs that should provoke Valgrind to give an error if there are any memory leaks or uninitialized values. This is described in further detail in \ref{appx:testing:valgrind}.

Finally, as described in \ref{appx:testing:travis}, we have instructed Travis CI to run these tests at every push to our GitHub repository so that the test results can be viewed for each version, eliminating the need to run them manually when looking for the last working version for instance.

All tests can be run from the root directory of this project with:

\begin{minted}[bgcolor=cbg_blue1]{text}
$ make test
\end{minted}

\subsection{Unit tests}
\label{appx:testing:unit_tests}
A unit test should test the smallest unit of the program, typically a single function. While we could load each C function from our library separately and test it, we have instead opted for unit testing the Python functions acting as interfaces to the underlying C library. Such testing is typically refered to as \emph{black box testing} -- we make no assumptions about the specific implementation, we only study the function from what output is yielded for various input.

The tests we will be performing are simple to write, yet they are able prove that our algorithm is correct.

If the algorithms satisfy the following properties, we can be fairly sure that our \emph{algorithm} is correct. There may still be programmatic errors such as memory leaks and bad type handling, but those are difficult to test without from a black box perspective.
\begin{enumerate}[label=(\roman*)]
    \item Running a single iteration of the algorithm yields new values $v^{l+1}$ which are related to the previous values $v^l$ as prescribed by the selected scheme. \label{lst:prop1}
    \item Running a single iteration two times, yield the same answer as running two iterations in one function call.\label{lst:prop2}
\end{enumerate}

Notice that this list of properties have the structure of a proof by induction. We first prove that our algorithm is correct for a single step $l = 1$, and then that running a single step after $l = n$ yields $l = n+1$.

While property \ref{lst:prop1} is a mandatory test to perform, it may be less obvious why property \ref{lst:prop2} is relevant from a programmatic point of view. Here we should note that our implementation of the algorithm works by storing a pair of arrays containing $v^l$ and $v^{l+1}$ at any given time step $l$. Instead of always writing into the array for $v^{l+1}$ and then copying its contents into $v^l$ at the end of the time step, we swap their pointers to avoid unneccesary memory writes. It is easy to make errors when performing such pointer swaps, but if we test with both an odd and an even number of iterations, we should be pretty confident that our implementation is correct.

These tests can be found in \program{src/tests.py} and they can be run with

\begin{minted}[bgcolor=cbg_blue1]{text}
$ make pytest
\end{minted}

\subsection{Doctests}
\label{appx:testing:doctests}
We have written a few doctests which are more of the sort where we look at the result and consider whether it is reasonable or not.

These tests can be found in \program{src/doctests.py} and they can be run with

\begin{minted}[bgcolor=cbg_blue1]{text}
$ make pytest
\end{minted}


\subsection{Testing for memory related errors with Valgrind}
\label{appx:testing:valgrind}
We wrote some minimal C programs which call our solvers so that we could run them with Valgrind to discover memory leaks and other memory related errors which may be hard to detect.

A particularly useful feature of Valgrind is that it can discover errors with uninitialized values, so we have designed our tests to initialize all variables properly and use the resulting $v$ array to compute a sum of all elements. Then we have an if statement which needs to compare that sum to a particular value. If not all elements of $v$ are properly initialized, this will result in Valgrind detecting a conditional jump depending on an uninitialized value.

These tests can be run from the root directory of this project with:

\begin{minted}[bgcolor=cbg_blue1]{text}
$ make memtest
\end{minted}

\subsection{Automating the testing with Travis CI}
\label{appx:testing:travis}
Travis CI is a web service which allows automated testing of software. It is integrated with GitHub in such a way that when we push to our repository, a build job is scheduled on Travis. An overview of the latest build status is available at \url{https://travis-ci.org/KGHustad/FYS3150}.

Travis allows us to perform regular and reproducible tests and logs the results for us. Due to its integration with GitHub, the \href{https://github.com/KGHustad/FYS3150/commits/master}{list of commits for our repository} shows are green tick or a red cross depending on whether the build passed or failed, respectively.

Moreover, one can test different combinations of software to ensure that it works for more than one configuration, so in principle, we could have tested our programs with Python 2.6, 2.7, 3.5, etc., however, we have just tested with Python 2.7 since that is the version we have been using.

We have configured Travis to run all the aforementioned tests in addition to compiling all the C code from this project as well as previous projects.
