
\section{Parallelisation of the 2D explicit scheme}
\label{appx:parallel}

In this appendix, we will analyse the isoefficiency of the parallel algorithms we discuss as described in \cite{inf3380_bok}. We will let $N$ denote the number of spatial points in along either axis (recall that $\dx = \dy$) and $\tau+1$ the number of temporal points.

The time needed to perform the computation serially is thus given by
\begin{equation}
T_S = \bigtheta(\tau N^2)
\end{equation}

%But since there is a strict dependence between the currect time step and the previous one, we could just as well study the time needed to compute a single time step (with the single communication).

\subsection{Work distribution and granularity}
With the explicit scheme, we can compute $v_{ij}^l$ for all $i, j \in \indexsetinner$, if all values at the previous time step are known. With Dirichlet boundary conditions, no action is required -- the boundary remains constant. Boundaries with Newman conditions do need to be updated, and this must be done after the corresponding inner point has been computed.


\subsubsection{2D decomposition}
\label{appx:parallel:analysis:2d}
Since there are $(N)^2$ independent values to be computed, the finest decomposition we could make, would be a 2D decomposition with $N^2$ workers. Such a decomposition would, however, be terribly inefficient due to the overhead incurred from the needed communication.

If one assumes that the values have already been distributed among the $\frac{N^2}{n^2}$ workers, and that each worker is responsible for computing $n^2$ values. This would lead to each worker having to communicate $4n$ values to its neighbour workers at each time step \footnote{The workers at the boundaries do not, of course, need to send the boundary points, but due to the implicit synchronisation occurring when the workers wait for the required data to arrive, this does not affect the overall time used.}.
If we further assume that the workers are connected in a 2D mesh network, so that each worker is directly connected to its 4 neighbours, then we see that we have the communication at each time step runs in $\bigtheta (n)$ time. The work required to compute the $n^2$ values is obviously $\bigtheta(n^2)$.

This yields a parallel time of
\begin{equation}
T_P = \bigtheta(\tau(n^2 + n)) = \bigtheta(\tau n^2)
\end{equation}

This means that we get a speedup of
\begin{equation}
S = \frac{T_S}{T_P} = \frac{\bigtheta(\tau N^2)}{\bigtheta(\tau n^2)}
= \bigtheta\left( \frac{N^2}{n^2} \right)
\end{equation}

Which is asymptotically equal to the number of workers, and hence this decomposition is cost-optimal!

\subsubsection{1D decomposition}
\label{appx:parallel:analysis:1d}
While the 2D decomposition is nice for use with supercomputers, it does carry some limitations. Obviously the number of workers $\frac{N^2}{n^2}$ would need to be an integer, but since $\frac{N^2}{n^2} = \left(\frac{N}{n}\right)^2$ we see that in fact it need not only be an integer, but also a perfect square! \footnote{It is possible to modify this decomposition so that the number of workers is a product of two (not necessarily equal) integers, but the efficiency degrades as the workers grid of values deviates from a square.} This is really bad news for anyone wanting to run our program on their CPU with 8 logical processors.

In order to simplify matters, we use a 1D decomposition (along the y-axis). We let each of the $\frac{N}{n}$ workers be responsible for computing $n$ rows. Each worker would then at each time step have to communicate its entire upper and lower row to the workers above and below, respectively. This would take $\bigtheta(N)$ time. The computation at each time step would take $\bigtheta(nN)$ time.

We see that with this leads to a parallel time of
\begin{equation}
T_P = \bigtheta(\tau(nN + N)) = \bigtheta(\tau nN)
\end{equation}

and a speedup of
\begin{equation}
S = \frac{T_S}{T_P} = \frac{\bigtheta(\tau N^2)}{\bigtheta(\tau nN)}
= \bigtheta\left( \frac{N}{n} \right)
\end{equation}

which is asymptotically equal to the number of workers, and hence this decomposition is cost-optimal too.


\subsection{Implementation of the parallel algorithm}
Since the 2D decomposition discussed in \ref{appx:parallel:analysis:2d} involves an order of magnitude more hassle, we chose to use the 1D decomposition discussed in \ref{appx:parallel:analysis:1d}.

We have made two implementations, one by extending our serial C code with OpenMP directives and one with explicit message passing with MPI.
Unfortunately, the part of the MPI code which carried out the data distribution was lost shortly after being written as one of authors' computer's motherboard fell into eternal sleep causing the encrypted data on the local drive to be impossible to recover.
