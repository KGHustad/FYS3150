\documentclass[10pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc,url}
\usepackage{parskip}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{verbatim}
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{physics}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage{epigraph}
\usepackage{hyperref}
\usepackage[toc,page]{appendix}
\usepackage{varioref}
\usepackage{enumitem}
\usepackage{minted}

\definecolor{cbg_blue1}{rgb}{0.87843, 0.95686, 1.0}

\newenvironment{code_block}[1]{
\begin{minted}[bgcolor=cbg_blue1]{#1}}
{\end{minted}}


% varioref stuff from Anders
\labelformat{section}{section~#1}
\labelformat{subsection}{section~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}


\newcommand{\program}[1]{\href{https://github.com/KGHustad/FYS3150/blob/master/project5/#1}{#1}}

\newcommand{\indexset}{\mathcal{I}}
\newcommand{\indexsetinner}{\mathcal{I}_{\mathrm{inner}}}
\newcommand{\bigO}{{\mathcal{O}}}
\newcommand{\bigtheta}{\Theta}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\dt}{{\Delta t}}
\newcommand{\dx}{{\Delta x}}
\newcommand{\dy}{{\Delta y}}
\newcommand{\du}{{\Delta u}}
\newcommand{\fracpt}{\frac{\partial}{\partial t}}
\newcommand{\fracpx}{\frac{\partial}{\partial x}}
\newcommand{\fracpy}{\frac{\partial}{\partial y}}
\newcommand{\fracpxx}{\frac{\partial^2}{\partial x^2}}
\newcommand{\fracpyy}{\frac{\partial^2}{\partial y^2}}
\newcommand{\pt}{{\partial t}}
\newcommand{\px}{{\partial x}}
\newcommand{\py}{{\partial y}}
\newcommand{\pu}{{\partial u}}
\newcommand{\ppu}{{\partial^2 u}}
\newcommand{\pppu}{{\partial^3 u}}

\begin{document}



\title{FYS3150 -- Project 5 -- PDE}
\author{
	\begin{tabular}{rl}
        Kristian Gregorius Hustad & (\texttt{krihus})\\
        Jonas Gahr Sturtzel Lunde & (\texttt{jonassl})
	\end{tabular}}
\date{December 9, 2016}
\maketitle



\setlength{\epigraphwidth}{0.75\textwidth}
\renewcommand{\epigraphflush}{center}
\renewcommand{\beforeepigraphskip}{50pt}
\renewcommand{\afterepigraphskip}{100pt}
\renewcommand{\epigraphsize}{\normalsize}
\epigraph{Nobody reads my lecture notes}
	{\textit{Morten Hjorth-Jensen}}

\begin{abstract}
\noindent
This is an abstract
\end{abstract}

\vfill


\begin{center}
    GitHub repository at \url{https://github.com/KGHustad/FYS3150}
    or \url{https://github.uio.no/krihus/FYS3150} (UiO mirror)
\end{center}


\pagebreak

\tableofcontents



\section{Introduction}
In this report, we wish to study partial differential equations, a powerful tool for accurately estimating the characteristics of complex systems over time. Our main focus will be on numerical solutions, as most PDEs are too complicated for analytical solutions. The methods of choice are three different finite difference schemes - Forward Euler, Backward Euler and the Crank-Nicolson scheme.
\\\\
We have, however, in order to verify our results, chosen an analytically solvable PDE. The problem we will be employing, is a simplified case of the \textit{diffusion equation}. This specific case will be derived in \vref{sec:dif}, and is shown in one and two spatial dimensions below.

\begin{equation}\label{eqn:PDE}
\frac{\ppu(x,t)}{\px^2} = \frac{\pu(x,t)}{\pt}
\end{equation}

\begin{equation}\label{eqn:PDE2}
\frac{\ppu(x,y,t)}{\px^2} + \frac{\ppu(x,y,t)}{\py^2} = \frac{\pu(x,t)}{\pt}
\end{equation}


The diffusion equation is, naturally, used to model the process of diffusion, where large systems of randomly moving particles are described by it's macroscopic behaviour. The quantity $u(\textbf{r},t)$ may be physically interpreted as, among other things, the particle density, energy density, or temperature, at a given point in time and space.
\\\\
\textbf{TODO: Write some more about how we wish to implement the differential equation numerically}



We shall see how the tridiagonal solver described in \cite{hustad_lunde_project1} can be used to solve the backward Euler and Crank-Nicolson schemes efficiently in one dimension.

\section{Method and Idea}

\subsection{Studying the diffusion equation}\label{sec:dif}

The diffusion equation is normally defined as
\begin{equation}\label{eqn:dif}
\nabla \cdot \left[D(u,\textbf{r}) \ \nabla u(\textbf{r},t)\right] = \frac{\pu(\textbf{r},t)}{\pt}
\end{equation}
where $D(u,\textbf{r})$ is the diffusion coefficient and \textbf{r} is the position vector.
\\
Assuming the diffusion coefficient to be constant, the diffusion equation \vref{eqn:dif} collapses down to the heat equation:
\begin{equation}
D \nabla^2 u(\textbf{r},t) = \frac{\pu(\textbf{r},t)}{\pt}
\end{equation}
Since we are not looking at a specific physical interpretation of the diffusion equation, $D$ is left as an unknown constant. We will further simplify the equation by scaling our variables such that they become dimensionless, and $D$ disappears. This leaves us with the partial differential equation we will study:
\begin{equation}\label{eq:dif_simple}
\nabla^2 u(\textbf{r},t) = \frac{\pu(\textbf{r},t)}{\pt}
\end{equation}
Writing out the Laplace-operator, this becomes the equations \ref{eqn:PDE} and \ref{eqn:PDE2}, shown in the introduction, where $x$, $y$, and $t$ now are dimensionless variables.

\subsubsection{Source term}
It is possible to extend \vref{eq:dif_simple} with a source term, $f(x, y, t)$, however, it is not necessary for the computations we will be doing, and we will therefore not discuss source terms in this report.



\subsection{Discretization \& Notation}\label{sec:disc}
Each spatial dimension will be discretized as a total of $n+2$ points.\\
In one dimension:
\begin{equation} (x_i), \quad i \in [0,n+1] \end{equation}
and in two dimensions:
\begin{equation} (x_i, y_j), \quad i,j \in [0,n+1]\end{equation}

The time will be discretized as $\tau+1$ points.
\begin{equation} t_l, \quad l \in [0,\tau] \end{equation}
We will also rewrite this to the following compact notation from \cite{hpl_fdm} where spatial position is written in subscript and the temporal position in superscript \\
In one dimension:
\begin{equation}
u(x+\dx,\ t+\dt) = u(x_{i+1},\ t_{l+1}) = u_{i+1}^{l+1}
\end{equation}
and two dimensions:
\begin{equation}
u(x+\dx,\ y+\dy,\ t+\dt) = u(x_{i+1},\ y_{j+1},\ t_{l+1}) = u_{i+1,j+1}^{l+1}
\end{equation}


We will also write the derivatives in \ref{eqn:PDE} and \ref{eqn:PDE2}Â with the compact notations
\begin{equation} u_{xx} = u_t\end{equation}
\begin{equation} \ u_{xx} + u_{yy} = u_t \end{equation}


\subsection{Initial Conditions \& Boundary Conditions}
The system will be interpreted as a rod of length $L$ for the one-dimensional system, and a square with side-lengths $L$ for the two-dimensional system, such that $x,y \in [0,L]$. We will study the system over a time $T$. This gives a length
\begin{equation}
\dx = \dy = x_i - x_{i-1} = y_j - y_{j-1} = \frac{L}{n+1}
\end{equation}
between the spatial discretizations, and a time
\begin{equation}
\dt = t_l - t_{l-1} = \frac{T}{\tau}
\end{equation}
between the time discretizations.\\\\
The initial conditions of the system are set as
\begin{equation}
u(x,t) = 0, \quad 0 < x < L
\end{equation}
while the boundary conditions of our system are
\begin{equation}
u(0,t) = 0 \quad \quad u(L,t) = 1
\end{equation}




\subsection{One dimensional case}\label{sec:method:1d}
\subsubsection{The central difference scheme}\label{sec:cent}
The Forward and Backward Euler schemes will be relying on the three-point central difference scheme to approximate the spatial derivative. The scheme is derived from a Taylor expansion around $u(x,t)$, with time as variable, both forward and backwards.
\begin{equation}
u(x+\Delta x, t) = u(x,t) + \frac{\pu(x,t)}{\px}\dx + \frac{\ppu(x,t)}{2\px^2}\dx^2 + O(\dx^3)
\end{equation}

\begin{equation}
u(x-\Delta x, t) = u(x,t) - \frac{\pu(x,t)}{\px}\dx + \frac{\ppu(x,t)}{2\px^2}\dx^2 + O(\dx^3)
\end{equation}

adding both sides of the equations together, we get
\begin{equation}
u(x+\Delta x, t) + u(x-\Delta x, t) = 2u(x,t) + 2\frac{\ppu(x,t)}{2\px^2}\dx^2 + 2O(\dx^3)
\end{equation}

which, solving for the second derivative of $x$, gives
\begin{equation}\begin{split}
\frac{\ppu(x,t)}{2\px^2} &= \frac{u(x+\Delta x, t) - 2u(x,t) + u(x-\Delta x, t)}{\dx^2} + \frac{2O(\dx^3)}{\dx^2}\\
&\approx \frac{u(x+\Delta x, t) - 2u(x,t) + u(x-\Delta x, t)}{\dx^2}
\end{split}\end{equation}
Giving a truncation error running as $\dx$.\\\\
We can discretize the derivative as shown in \ref{sec:disc}.
\begin{equation}
u_{xx} = \frac{u_{i+1}^l - 2u_i^l+u_{i-1}^l}{\dx^2}
\end{equation}




\subsubsection{Forward Euler - Explicit scheme}
We will now derive the Explicit Forward Euler method, to approximate the partial differential equation \ref{eqn:PDE}. By Taylor expanding forward around $u(x,t)$, with time as variable, and truncating after the first derivative, we get
\begin{equation}
u(x,t+\dt) = u(x,t) + \frac{\pu(x,t)}{\pt}\dt + O(\dt^2)
\end{equation}
Solving for the time derivative gives
\begin{equation}
\frac{\pu(x,t)}{\pt} = \frac{u(x,t+\dt) - u(x,t)}{\dt} + \frac{O(\dt^2)}{\dt} \approx \frac{u(x,t+\dt) - u(x,t)}{\dt}
\end{equation}
which means we have a truncation error running as $\dt$.
\\\\
We can discretize this as shown in \ref{sec:disc}, giving
\begin{equation}
u_t = \frac{u_i^{l+1} - u_i^l}{\dt}
\end{equation}
Combining this with the central difference scheme, we get the approximation to the partial differential equation \vref{eqn:PDE}
\begin{equation}
\frac{u_{i+1}^l - 2u_i^l+u_{i-1}^l}{\dx^2} = \frac{u_i^{l+1} - u_i^l}{\dt}
\end{equation}

Solving for $u_i^{l+1}$, and introducing the known constant $\alpha = \frac{\dt}{\dx^2}$, we arrive at the explicit scheme
\begin{equation}\label{eq:1dscheme:fe}
u_i^{l+1} = \alpha u_{i-1}^l + (1-2\alpha)u_i^l + \alpha u_{i+1}^l
\end{equation}
We see that the state of the system at a time, $t_{l+1}$(left side), only depends on the conditions of the system in a previous state, $t_l$(right side), making this an explicit method.


\subsubsection{Backward Euler - Implicit scheme}\label{sec:method_be}
We will now derive the implicit Backward Euler method to approximate the time derivative of the partial differential equation. Taylor expanding backwards around $u(x,t)$, truncating after the first derivative, we get

\begin{equation}
u(x,t-\dt) = u(x,t) - \frac{\pu(x,t)}{\pt}\dt + O(\dt^2)
\end{equation}
Solving for the time derivative gives
\begin{equation}
\frac{\pu(x,t)}{\pt} = \frac{u(x,t) - u(x,t-\dt)}{\dt} + \frac{O(\dt^2)}{\dt} \approx \frac{u(x,t) - u(x,t-\dt)}{\dt}
\end{equation}
giving a truncation error running as $\dt$, just as the explicit scheme.
\\\\
We can discretize this as shown in \ref{sec:disc}, giving
\begin{equation}
u_t = \frac{u_i^l - u_i^{l-1}}{\dt}
\end{equation}
Again combining with the central difference scheme, we approximate the partial differential equation as
\begin{equation}
\frac{u_{i+1}^l - 2u_i^l+u_{i-1}^l}{\dx^2} = \frac{u_i^l - u_i^{l-1}}{\dt}
\end{equation}
Lastly, introducing $\alpha = \frac{\dt}{\dx^2}$, we can rewrite to
\begin{equation}\label{eq:1dscheme:be}
u_i^{l-1} = -\alpha u_{i+1}^l + (1 + 2\alpha )u_i^l - \alpha u_{i-1}^l
\end{equation}
\\
We see that we cannot write the state of the system as an explicit function of a previous state, making this an implicit scheme. If we write \ref{eq:1dscheme:be} out for all values of $i \in [0,n+1]$, we see that it becomes a set of linear equations, which we will write as the matrix equation
\begin{equation}\label{eq:be_matrix}
V_{l-1} = \hat{A}V_l
\end{equation}
where we define the three-diagonal matrix:
\begin{equation}
\hat{A} = \begin{bmatrix}
1+2\alpha & -\alpha & 0 & \cdots & \cdots \\
-\alpha & 1+2\alpha & -\alpha & \cdots & \vdots \\
0 & \ddots & \ddots & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & -\alpha \\
\vdots & \cdots & \cdots & -\alpha & 1+2\alpha \\
\end{bmatrix}
\end{equation}
and the state of the system at a given time $t_l$:
\begin{equation}\label{eqn:V} V_l = \begin{bmatrix}
u_{0}^l\\
u_{1}^l\\
\cdots\\
u_{n}^l\\
u_{n+1}^l\\
\end{bmatrix}
\end{equation}

This means we can define the state of the system, $V_l$, at time, $t_l$, as
\begin{equation} \label{eq:be_matrix_inverse}
V_l = \hat{A}^{-l}V_0
\end{equation}

While we could implement our algorithm using \ref{eq:be_matrix_inverse}, we will be using \ref{eq:be_matrix} so that we can employ our tridiagonal solver.

\subsubsection{The Crank-Nicolson scheme}
\textbf{TODO}: Deriving Crank-Nicolson scheme more thoroughly, among other tings: showing the truncation's error.

While the Forward and Backward Euler methods rely on taking a step forwards and backwards in time to approximate the derivative, the Crank-Nicolson scheme simply takes the average of the two. Moving the Backward Euler scheme one time step ahead, such that it's time derivative matches the Forward Euler's, we get the average:
\begin{equation}
\frac{u_i^{l+1}-u_i^l}{\dt} = \frac{1}{2}\left( \frac{u_{i+1}^l - 2u_i^l + u_{i-1}^l}{\dx^2} + \frac{u_{i+1}^{l+1} - 2u_i^{l+1} + u_{i-1}^{l+1}}{\dx^2}\right)
\end{equation}
After again defining $\alpha = \frac{\dt}{\dx^2}$, we can separate the two time steps $t_l$ and $t_{l-1}$, giving us the implicit Crank-Nicolson scheme.
\begin{equation}
-\alpha u_{i-1}^l + (2+2\alpha)u_i^l - \alpha u_{i+1}^l = \alpha u_{i-1}^{l-1} + (2-2\alpha)u_i^{l-1} + \alpha u_{i+1}^{l-1}
\end{equation}
As with the Backward Euler scheme, we write this as a system of linear equations for $i \inÂ [0,n+1]$, which can be written as the matrix equation
\begin{equation}\label{eqn:crank}
(2\hat{I}+\alpha \hat{B})V_l = (2\hat{I}-\alpha \hat{B})V_{l-1}
\end{equation}
where $V_l$ is defined in \vref{eqn:V}. We also have the identity matrix $\hat{I}$, and the three-diagonal matrix:

\[
\hat{B} = \begin{bmatrix}
2 & -1 & 0 & \cdots & \cdots \\
-1 & 2 & -1 & 0 & \vdots \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
\vdots & \ddots & \ddots & 2 & -1 \\
\vdots & \cdots & 0 & -1 & 2 \\
\end{bmatrix}
\]

We can rewrite \vref{eqn:crank} to define the state of the system $V_l$ as a function of it's previous state $V_{l-1}$:
\begin{equation}
V_l = (2\hat{I}+\alpha \hat{B})^{-1} (2\hat{I} - \alpha \hat{B})V_{l-1}
\end{equation}

Just as in \vref{sec:method_be} we will be using our tridiagonal solver instead of performing a matrix inversion. To see that this is indeed possible, observe that the only unknown in \vref{eqn:crank} is $V_l$, so it can be written as a matrix equation \[
A \mathbf{v} = \mathbf{s}
\] where $A = 2\hat{I}+\alpha \hat{B}$, $ \mathbf{s} = (2\hat{I} - \alpha \hat{B})V_{l-1}$ and $\mathbf{v} = V_l$.



\subsubsection{An analytical solution}\label{sec:analytical_1d}
\cite{inf-mat2351_book} gives a solution to the diffusion equation

\begin{align}\label{eq:analytical_1d_book}
    u(x, t) = e^{-\pi^2 t} \sin(\pi x) , \quad u(0, t) = u(1, t) = 0
\end{align}

We can easily show that \vref{eq:analytical_1d_book} solves \vref{eqn:PDE},
\begin{align}
\fracpt u(x, t) &= -\pi^2 u(x, t) \\
\fracpxx u(x, t) &= \fracpx \left(\pi e^{-\pi^2 t} \cos(\pi x) \right)
= -\pi^2 e^{-\pi^2 t} \sin(\pi x) = -\pi^2 u(x, t)
\end{align}



\subsection{Error and stability}
\label{sec:method:1d:error}
Since all methods build on the central difference scheme, the truncation error of the spatial derivative runs as $\dx^2$ for all schemes. From deriving the three methods in \ref{sec:method:1d} we know that the Euler methods has a truncation error running as $\dt$, while it is $\dt^2$ for the Crank-Nicolson scheme.
\\\\



\begin{table}[htb]
\begin{center}
\begin{tabular}{lcc}
 		& Truncation Error	& Stability
\\ \hline \\
Forward Euler	& $\dt$ and $\dx^2$	& only $\dt \le \frac{1}{2}\dx^2$ \\
\\ \hline \\
Backward Euler	& $\dt$ and $\dx^2$	& all $\dt$ and $\dx$ \\
\\ \hline \\
Crank-Nicolson	& $\dt^2$ and $\dx^2$	& all $\dt$ and $\dx$ \\
\\ \hline \\
\end{tabular}
\end{center}
\caption{Proportionality of truncation error and stability requirements for the three numerical schemes}
\label{table:error}
\end{table}


\subsection{Two dimensional case}
We will now expand our implementations to two spatial dimensions, resulting in the partial differential equation, $u_{xx} + u_{yy} = u_t$, or, written out:
\begin{equation}
\left(\frac{\ppu(x,y,t)}{\px^2} + \frac{\ppu(x,y,t)}{\py^2}\right) = \frac{\pu(x,y,t)}{\pt}
\label{eq:diffusion_2d}
\end{equation}
The central difference scheme derived for one dimension in \ref{sec:cent} can easily be expanded to two spatial dimensions, giving the left side of the PDE:
\begin{equation}
u_{xx} + u_{yy} = \frac{u_{i+1,j}^l - 2u_i^l+u_{i-1,j}^l}{\dx^2} + \frac{u_{i,j+1}^l - 2u_i^l+u_{i,j-1}^l}{\dy^2}
\end{equation}

As we have only introduced a new spatial dimension, the schemes for approximating time derivatives remains unchanged. We expand the notation of the Forward Euler method to two dimensions:
\begin{equation}
u_t = \frac{u_{i,j}^{l+1} - u_{i,j}^l}{\dt}
\end{equation}

Choosing $\dx = \dy = h$, and combining these two schemes, we can set up the two dimensional partial differential equation
\begin{equation}
\frac{u_{i+1,j}^l - 2u_{i,j}^l + u_{i-1,j}^l}{h^2} + \frac{u_{i,j+1}^l - 2u_{i,j}^l + u_{i,j-1}^l}{h^2} = \frac{u_{i,j}^{l+1} - u_{i,j}^l}{\dt}
\end{equation}

Solving for $u_{i,j}^{l+1}$ gives us the explicit scheme
\begin{equation}
u_{i,j}^{l+1} = u_{i,j}^l + \alpha\left( u_{i+1,j}^l + u_{i-1,j}^l + u_{i,j+1}^l + u_{i,j-1}^l - 4u_{i,j}^l \right)
\end{equation}
where $\alpha = \frac{\dt}{h^2}$.


\subsubsection{An analytical solution}
We can adapt the analytical solution described in \vref{sec:analytical_1d} to solve the 2D problem.

Using the Dirichlet boundary conditions
\begin{align}
u(0, y, t) = u(1, y, t) = u(x, 0, t) = u(x, 1, t) = 0
\end{align}
we get
\begin{align}\label{eq:analytical_2d}
    u(x, y, t) = e^{-2\pi^2 t} \sin(\pi x) \sin(\pi y)
\end{align}

which solves \vref{eq:diffusion_2d} since
\begin{align}
\fracpt u(x, y, t) &= -2\pi^2 u(x, y, t) \\
\fracpxx u(x, y, t) &= \fracpx \left( \pi e^{-2\pi^2 t} \cos(\pi x) \sin(\pi y) \right) \\
&= -\pi^2 e^{-2\pi^2 t} \sin(\pi x) \sin(\pi y) \\
&= -\pi^2 u(x, y, t) \\
\fracpyy u(x, y, t) &= \fracpy \left( \pi e^{-2\pi^2 t} \sin(\pi x) \cos(\pi y) \right) \\
&= -\pi^2 e^{-2\pi^2 t} \sin(\pi x) \sin(\pi y) \\
&= -\pi^2 u(x, y, t)
\end{align}

\section{Implementation and results}\label{sec:implementation_and_results}
Note: $\dt$ is always set to $\frac{1}{2}\dx^2$ unless otherwise mentioned.\\\\

\subsection{Sinus case}
We will study how the various schemes by comparing them to the analytical solution derived in \ref{sec:analytical_1d}. However, before looking at the results, we should remind ourselves of how the methods differ and what we should expect.

First we should note that our analytical solution, \ref{eq:analytical_1d_book}, has the shape of a sine wave that decays over time. As time approaches infinity, i.e. $\lim_{t \to \infty} u(x, t) = 0$.

Forward Euler uses a forward difference in time, meaning it approximates the rate of change in the next time step from rate of change in the current time step. Since the analytical solution is decaying, this means that Forward Euler is at an advantage in terms of convergence rate.

Backward Euler, which uses a backward difference in time, makes the opposite assumption. The rate of change at the current time step is approximated from the rate of change at the next time step. We will see that this is disadvantageous.

Crank-Nicolson strikes a balance between Forward and Backward Euler. It does, however have a faster convergence rate in terms of $\dt$ as discussed in \ref{sec:method:1d:error}.

We clearly see the impact of the decreased $\dt$ value from 0.1 to 0.01.
\begin{figure}[H]
\includegraphics[width=\textwidth]{fig/{plot_T=0.10}.pdf}
\caption{Different PDE solvers with $\dx \in \{0.01,0.1\}$ plotted against analytical solution for $T=1$.}
\label{fig:}
\end{figure}

Same scenario as above, this time after ten times as long. Now the impact of the decreased $\dx$ is even clearer. We also see Forward Euler diverging the quickest, due to the nature of the problem. This is not always the case, as the Crank-Nicolson is usually more accurate.
\begin{figure}[H]
\includegraphics[width=\textwidth]{fig/{plot_T=1.00}.pdf}
\caption{Different PDE solvers with $\dx \in \{0.01,0.1\}$ plotted against analytical solution for $T=0.1$.}
\label{fig:}
\end{figure}

Here we see what we suspected from the last plot: the Forward Euler actually has the quickest convergence.
\begin{figure}[H]
\includegraphics[width=\textwidth]{fig/{error_T=1.00n=10}.pdf}
\caption{Absolute error of different PDE solvers for the $T=0.1$, $\dx=0.1$, $\dt=0.0005$ case.}
\label{fig:}
\end{figure}

\subsection{Linear case}
In this specific case we are not capable of distinguishing the different schemes on a plot, as they are behave much alike.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{fig/{1D_linplot_T=0.10dx=0.10}.pdf}
\includegraphics[width=0.5\textwidth]{fig/{1D_linplot_T=0.10dx=0.01}.pdf}
\caption{Comparing $\dx = 0.1$ and $\dx = 0.01$ for the linear case.}
\label{fig:}
\end{figure}


Picking one of the methods, the Crank-Nicolson, we study how it diverges (we don't have an analytical solution here, but we know that it should converge on a linear solution).
\begin{figure}[H]
\includegraphics[width=\textwidth]{fig/{1D_linplot_Crank_nicolson}.pdf}
\caption{}
\label{fig:}
\end{figure}

\input{chapters/notes_on_implementation.tex}

\bibliographystyle{IEEEtran}
\bibliography{../papers}{}

\newpage
\appendix
\labelformat{section}{appendix~#1}
\input{chapters/appx_testing.tex}

\end{document}
