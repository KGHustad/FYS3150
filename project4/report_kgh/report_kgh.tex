\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc,url}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsmath, amssymb}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{parskip}
\usepackage{lmodern}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{epigraph}
\usepackage{listings}
\usepackage{physics}
\usepackage{varioref}
\usepackage{placeins}

% varioref stuff from Anders
\labelformat{section}{section~#1}
\labelformat{subsection}{section~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}



\begin{document}
\title{FYS3150 -- Project 4}
\author{
    \begin{tabular}{r l}
        Kristian Gregorius Hustad & (\texttt{krihus})
    \end{tabular}}
%\date{}    % if commented out, the date is set to the current date

\maketitle




% quote
\setlength{\epigraphwidth}{0.75\textwidth}
\renewcommand{\epigraphflush}{center}
\renewcommand{\beforeepigraphskip}{50pt}
\renewcommand{\afterepigraphskip}{100pt}
\renewcommand{\epigraphsize}{\normalsize}

\epigraph{Nobody actually creates perfect code the first time around, except me.}
{\textit{Linus Torvalds}}

% alternative quote
%\epigraph{The first principle is that you must not fool yourself -- and you are the easiest person to fool.}{\textit{Richard Feynman}}

\begin{abstract}
\noindent
In this report, we study the Ising model for a square lattice. We compare analytical and numerical results, we study the efficiency of our program and the Metropolis algorithm, and we verify numerically Lars Onsager's analytical solution for the critical heat of an infinitely large lattice.
\end{abstract}

\vfill

\textbf{NOTE:} All programs and derivations used in them were made in collaboration with Jonas Gahr Sturtzel Lunde (\texttt{jonassl}).

\bigskip
\begin{center}
    GitHub repository at \url{https://github.com/KGHustad/FYS3150}
\end{center}

\newpage

%%% MACROS
\newcommand{\half}{\frac{1}{2}}
\newcommand{\dt}{{\Delta t}}
\newcommand{\dx}{{\Delta x}}
\newcommand{\bigO}{{\mathcal{O}}}

\newcommand{\stateset}{{\mathcal{S}}}
\newcommand{\expectation}[1]{{\langle #1 \rangle}}

\newcommand{\upspin}{\uparrow}
\newcommand{\downspin}{{\color{red}\downarrow}}




\section{Introduction}\label{sec:intro}
%\subsection*{Description of the nature of the problem}
%\cite{mhj_lecture_notes} % must cite something to avoid compilation error when using bibtex

We aim to study the Ising model in for a square $L \times L$ lattice with periodic boundary conditions. We study the case of $L=2$ and compare analytical and numerical values. We then observe the convergence rate for $L=20$, and we look at the distribution of energies after reaching the steady state. Finally, we attempt to approximate the critical temperature as $L$ tends to infinity.


Methods are derived in \ref{sec:methods}, implementation considerations and results are given in \ref{sec:implementation_and_results}, and finally conclusions are drawn in \ref{sec:conclusion}.



\section{Discussion of methods}\label{sec:methods}
We have the following expression for the energy of a given state, $i$,
\begin{equation}
    E_i=-J\sum_{< kl >}s_k s_l
\end{equation}

and its mean magnetization by
\begin{equation}
    M_i=\sum_{k}^{L^2} s_k
\end{equation}

where $s_k, s_l$ are individual spins.

Further, we have the partition function, which is the sum of the energy for all states. We will denote the set of all possible states by $\stateset$.

\begin{equation}
    Z = \sum_{i \in \stateset}e^{\beta E_i}
\end{equation}

In general, a $L \times L$ lattice has $2^{L^2}$ possible states, i.e. $|\stateset| = 2^{L^2}$

\subsection{The case of $L = 2$}

\subsubsection{Energy and mean magnetization}

 We will study the case of $L = 2$ and find analycal expressions, which we will later compare to our numerical results.
\begin{table}[htb]
\begin{center}
\begin{tabular}{cccc}
    State & Symmetries & Energy ($J$) & Mean magnetization \\ \hline
    $ \begin{array}{cc}
        \upspin&\upspin \\
        \upspin&\upspin
    \end{array} $ &
    1 & % symmetries
    -8 & % energy
    4   % magnetization
    \\ \hline
    $ \begin{array}{cc}
        \upspin&\upspin \\
        \upspin&\downspin
    \end{array} $ &
    4 & % symmetries
    0 & % energy
    2   % magnetization
    \\ \hline
    $ \begin{array}{cc}
        \upspin&\upspin \\
        \downspin&\downspin
    \end{array} $ &
    4 & % symmetries
    0 & % energy
    0   % magnetization
    \\ \hline
    $ \begin{array}{cc}
        \upspin&\downspin \\
        \downspin&\upspin
    \end{array} $ &
    2 & % symmetries
    8 & % energy
    0   % magnetization
    \\ \hline
    $ \begin{array}{cc}
        \downspin&\downspin \\
        \downspin&\upspin
    \end{array} $ &
    4 & % symmetries
    0 & % energy
    -2   % magnetization
    \\ \hline
    $ \begin{array}{cc}
        \downspin&\downspin \\
        \downspin&\downspin
    \end{array} $ &
    1 & % symmetries
    -8 & % energy
    -4   % magnetization
    \\ \hline
\end{tabular}
\end{center}
\caption{All 16 possible states for $L=2$}
\label{table:states}
\end{table}

\subsubsection{Expectation values}

It is known that the expectation value for the energy, the energy squared, the absolute magnetization \footnote{We will not disciminate between positive and negative magnetization}, the heat capacity and the magnetic susceptibility are respectively

\begin{align}
    \langle E \rangle &= - \frac{1}{Z} \frac{\partial }{\partial \beta} Z \label{eq:mu_E} \\
    \langle E^2 \rangle &= \frac{1}{Z} \frac{\partial^2}{\partial \beta^2} Z \label{eq:mu_E_sq}\\
    \langle M \rangle &= \frac{1}{Z} \sum_{i \in \stateset} M_i e^{-\beta E_i} \label{eq:mu_M}\\
    \langle M^2 \rangle &= \frac{1}{Z} \sum_{i \in \stateset} M_i^2 e^{-\beta E_i} \label{eq:mu_M_sq}\\
    \langle C_V \rangle &= \frac{\langle E^2 \rangle - {\langle E \rangle}^2}{T^2} \label{eq:mu_C_V}\\
    \langle \chi \rangle &= \frac{\langle M^2 \rangle - {\langle M \rangle}^2}{T^2} \label{eq:mu_chi}
\end{align}

Using the energy values from \vref{table:states}, we obtain
\begin{align}
    Z = \sum_{i \in \stateset}e^{\beta E_i}
    &= 2e^{-8 J \beta} + 2e^{8 J \beta} + 12  \nonumber \\
    &= 2(e^{-8 J \beta} + e^{8 J \beta} + 6)
\end{align}

We can insert $Z$ into \vref{eq:mu_E} and \vref{eq:mu_E_sq} to obtain
\begin{align}
    \frac{\partial }{\partial \beta} Z &= -16J e^{-8 J \beta} + 16J e^{8J \beta} \\
    \frac{\partial^2}{\partial \beta^2} Z &= 128J^2 e^{-8 J \beta} + 128J^2 e^{8J \beta} \\
    \langle E \rangle &= - \frac{-16J e^{-8 J \beta} + 16J e^{8J \beta}}{2(e^{-8 J \beta} + e^{8 J \beta} + 6)}
    =  \frac{8J e^{-8 J \beta} - 8J e^{8J \beta}}{ e^{-8 J \beta} + e^{8 J \beta} + 6} \label{eq:mu_E:2} \\
    \langle E^2 \rangle &= \frac{128J^2 e^{-8 J \beta} + 128J^2 e^{8J \beta}}{2(e^{-8 J \beta} + e^{8 J \beta} + 6)}
    = \frac{64J^2 e^{-8 J \beta} + 64J^2 e^{8J \beta}}{e^{-8 J \beta} + e^{8 J \beta} + 6} \label{eq:mu_E_sq:2}
\end{align}

Likewise, we can use the magnetization values from \vref{table:states} to obtain
\begin{align}
    \langle M \rangle &= \frac{1}{Z} \sum_{i \in \stateset} M_i e^{-\beta E_i}
    = \frac{1}{Z} \left( 8 e^{8 J \beta} + 16 \right)
    = \frac{4 e^{8 J \beta} + 8}{e^{-8 J \beta} + e^{8 J \beta} + 6}
    \label{eq:mu_M:2} \\
    \langle M^2 \rangle &= \frac{1}{Z} \sum_{i \in \stateset} M_i^2 e^{-\beta E_i}
    = \frac{1}{Z} \left(32 e^{8 J \beta} + 32 \right)
    = \frac{16 e^{8 J \beta} + 16}{e^{-8 J \beta} + e^{8 J \beta} + 6}
    \label{eq:mu_M_sq:2}
\end{align}

We now have all we need to find $\expectation{\chi}$ and $\expectation{C_V}$, but we will not bother with writing out the expressions here.

\section{Implementation and results}\label{sec:implementation_and_results}
\subsection{Choice of language}
For our implementation, we chose a hybrid Python-C approach -- inexpensive operations such as initialization of arrays and extracting the important quantities from the results are done in Python, where we can write short high-level code, while the expensive operations, i.e. the Metropolis algorithm, is carried out in fast C code.

\subsection{Random number generation}
We use the GNU Scientific Library's \href{https://www.gnu.org/software/gsl/manual/html_node/Random-number-generator-algorithms.html}{implementation of the Mersenne-Twister algorithm} for our random number generation. This algorithm satisfies the desired statistical properties and is reasonably efficient\footnote{A minimal benchmark program can be found in \texttt{src/c/bench\_random.c}}.

The programs allow the user to fix the seed to a given value. If no seed is provided, the RNG will be seeded from \texttt{/dev/urandom}, which is a special file on UNIX-like systems from which random bytes can be read in a thread-safe manner, such that only a single thread can read a byte before a new one is generated.


\subsection{Parallelization}
The Metropolis algorithm cannot easily (and efficiently) be parallelized without side-effects\footnote{One could be tempted to operate with multiple threads on a single spin matrix, but the issue of maintaining cache coherency would arise, unless, of course, the threads worked on strictly independent parts of the spin matrix, in which case some border spins cannot be flipped.  \href{https://github.com/CompPhysics/ComputationalPhysics/blob/082ee7e/doc/Programs/ParallelizationOpenMP/OpenMPising.cpp}{This program} suffers from the former issue.}. One could combine several runs with different seeds to get some kind of average, but such an approach is sub-optimal. However, it is trivial to parallelize multiple runs with differing parameters.

For this purpose, we could have extended our C code with OpenMP or used MPI from Python (which has been shown to achieve similar efficiency to MPI from C++ in \cite{mortensen_langtangen_hpc_python}) but neither OpenMP nor MPI are optimal choices for such a problem. A pool-based parallelization model allows for efficient and automated distribution of tasks at run time. The pool size is usually chosen to be equal to the number of logical processors, and then each worker in the pool will fetch and compute tasks until the pool is empty. We used python's built-in \href{https://docs.python.org/2/library/multiprocessing.html}{multiprocessing} package, which provides a simple API to pool-based parallel processing.

Studying running times, we should not be surprised to find that our parallel program induces no measurable overhead \footnote{Strictly speaking, this is only the case for machines that are able to sustain peak clock speed on all cores. Also, hyperthreading is not true parallelism, so in general one cannot expect a speedup greater than the number of cores. For these reasons, a typical laptop will see a higher time usage per job when running in parallel, but that does not imply that our program is suboptimal.} and is hence optimal\footnote{We ran our program with 404 tasks on a machine with 64 logical processors. The computation which completed took 22566 CPU minutes and completed in 357 (wall clock) minutes, meaning 63.12/64 of the time was spent on useful computation (and the rest was spend idling).}.

\subsection{Comparing analytical and numerical results for $L=2$}
\begin{table}
    \input{table_b.tex}
    \caption{Relative error of numerical results for $N$ sweeps and $T=1$}
    \label{table:b}
\end{table}

From \ref{table:b} we see that $\chi$ and $C_V$ converges slower than the other quantities. We also see no improvement going from $N=10^{6}$ to $N=10^{7}$. It appears that $10^6$ sweeps over the lattice is sufficient to reach convergence.

\subsection{Rate of convergence}
After about $2\cdot 10^5$ sweeps over the lattice, the expectation values seem to have converged. We run our simulations with different input matrices but the same random seed, which mean that the same spins are attempted to be flipped. It appears that sometime before $2\cdot 10^5$, the spin matrices become equal, and from that point on, they will remain equal. Therefore, we can observe that figure \ref{fig:c:2.4:homogeneous} and figure \vref{fig:c:2.4:random} are identical.

\begin{figure}[htb]
%\includegraphics[width=\textwidth]{../fig/{plot_c_energy_homogeneous_T=2.4_sweeps=1E+06}.pdf}
\begin{center}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=\textwidth]{../fig/{plot_c_energy_homogeneous_T=2.4_sweeps=1E+06}.pdf}
\caption{Homogeneous}
\label{fig:c:2.4:homogeneous}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=\textwidth]{../fig/{plot_c_energy_random_T=2.4_sweeps=1E+06}.pdf}
\caption{Random}
\label{fig:c:2.4:random}
\end{subfigure}
\end{center}

\caption{Convergence of the expectation value for the energy}
\label{fig:c:2.4}
\end{figure}

When the steady state has been reached, we expect the rate of accepted configurations to decline, while the rate is particularly large for a random matrix which is far for the steady state.

For higher values temperatures, one would expect more chaos, and thus more accepted configurations. We see from \vref{table:accepted_configurations} that this is indeed the case.

\begin{table}[htb]
    \begin{center}
    \begin{tabular}{crr}
        T & Homogeneous & Random \\ \hline
        1.0 & 28606 & 30558 \\ \hline
        2.4 & 10864256 & 10875744 \\ \hline
    \end{tabular}
    \end{center}
    \caption{Accepted configurations for $L=2$ after $10^7$ sweeps}
    \label{table:accepted_configurations}
\end{table}

\subsection{Probability distributions}
Looking at the distribution of the energy states for $T=1$, we see that firuge \vref{fig:d:1} is heavily skewed towards the left -- almost all the values are all the way to the left. The variance is only about 9.3, which is very reasonable, when we look at the plot.

For $T=2.4$, the mean is far from the theoretical minimum energy, and as we see in figure \vref{fig:d:2.4}, the distribution of the energies is approximately normal distributed. We find a variance of about 3200, and which is also very reasonable. We could probably get a reasonably good fit by using the mean energy and variance with a normal distribution.

\begin{figure}[htb]
%\includegraphics[width=\textwidth]{../fig/{plot_c_energy_homogeneous_T=2.4_sweeps=1E+06}.pdf}
\begin{center}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=\textwidth]{../fig/{plot_d_T=1}.pdf}
\caption{$T=1$}
\label{fig:d:1}
\end{subfigure}
\begin{subfigure}[t]{0.8\textwidth}
\includegraphics[width=\textwidth]{../fig/{plot_d_T=2.4}.pdf}
\caption{$T=2.4$}
\label{fig:d:2.4}
\end{subfigure}
\end{center}

\caption{Distribution of the energies after a steady state has been reached with $L = 20$}
\label{fig:d}
\end{figure}


\subsection{Critical temperature}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{{../fig/plot_e_dT=0.001_sweeps=2E+06_specific_heat}.pdf}
\caption{The expectation value for $C_V$ of the sweeps $\in [10^6, 2\cdot10^6]$ with $\Delta T = 0.001$}
\label{fig:e}
\end{figure}

From \vref{fig:e}, we see that the peaks seem to converge towards the analytical critical temperature, $\frac{2}{\ln(1 + \sqrt{2})} \approx 2.269$.

We have

\begin{equation}
    T_C(L)-T_C(L=\infty) = aL^{-1/\nu}
    \label{eq:critical:infty}
\end{equation}


Inserting for two values $L_i, L_j$ in \ref{eq:critical:infty} and subtracting, we obtain
\begin{align}
    T_C(L_i) - T_C(L_j) &= a(L_i^{-1/\nu} - L_j^{-1/\nu}) \nonumber \\
    a &= \frac{T_C(L_i) - T_C(L_j)}{L_i^{-1/\nu} - L_j^{-1/\nu}}
    \label{eq:critical:pair}
\end{align}

Now, selecting our approximations to $T_C(L)$ is somewhat difficult with the significant fluctuations, but a simple solution would be to select the $T$ value corrensponding to the global maximum for each $L$. Doing this, we find $T_C(100) = 2.277$ and $T_C(140) = 2.275$.

We set $\nu=1$. Inserting $L_i = 100$ and $L_j = 140$ in \ref{eq:critical:pair}, we approximate $a$ to
\begin{align}
    a &= \frac{T_C(L_i) - T_C(L_j)}{L_i^{-1} - L_j^{-1}} \nonumber \\
     &= \frac{T_C(100) - T_C(140)}{\frac{1}{100} - \frac{1}{140}}
     \approx 0.7
\end{align}

Rearranging \ref{eq:critical:infty}, and inserting $L=140$ we find
\begin{align}
    {T_C}_\infty &= T_C(L) - \frac{a}{L} \nonumber \\
    &= 2.275 - \frac{0.7}{140}
    = 2.270
\end{align}

We are quite content with an error of 0.001 from the analytical value.

\section{Conclusion}\label{sec:conclusion}
We have found an agreement betweeen the analytical solutions and our program for $L=2$, we saw that the quite a lot of sweeps are needed to achieve convergence, and we found our approximated critical heat to be in agreement with the analytical solutions of Lars Onsager.

%\bibliographystyle{plain}
%\bibliographystyle{siam}
\bibliographystyle{IEEEtran}
\bibliography{../../papers}{}

\FloatBarrier


\section{Appendix}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{{../fig/plot_e_dT=0.001_sweeps=2E+06_susceptibility}.pdf}
\caption{The expectation value for $\chi$ of the sweeps $\in [10^6, 2\cdot10^6]$ with $\Delta T = 0.001$}
\end{figure}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{{../fig/plot_e_dT=0.001_sweeps=2E+06_mean_energy}.pdf}
\caption{The expectation value for the mean energy of the sweeps $\in [10^6, 2\cdot10^6]$ with $\Delta T = 0.001$}
\end{figure}

\begin{figure}[htb]
\includegraphics[width=\textwidth]{{../fig/plot_e_dT=0.001_sweeps=2E+06_mean_abs_magnetization}.pdf}
\caption{The expectation value for the absolute magnetization of the sweeps $\in [10^6, 2\cdot10^6]$ with $\Delta T = 0.001$}
\end{figure}

\end{document}
